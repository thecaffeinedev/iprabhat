<!DOCTYPE html> <html lang="en-US"> <head prefix="og: http://ogp.me/ns#"> <meta charset="UTF-8" /> <meta http-equiv="X-UA-Compatible" content="ie=edge" /> <meta name="viewport" content="width=device-width, initial-scale=1.0" /> <meta name="mobile-web-app-capable" content="yes" /> <meta name="apple-mobile-web-app-capable" content="yes" /> <meta name="application-name" content="Prabhat" /> <meta name="apple-mobile-web-app-status-bar-style" content="#fff" /> <meta name="apple-mobile-web-app-title" content="Prabhat" /> <title> Custom Object Detection With YoloV3 - Prabhat </title> <link rel="alternate" href="http://localhost:4000/Custom-Object-Detection-With-YoloV3/" hreflang="en-US" /> <link rel="canonical" href="http://localhost:4000/Custom-Object-Detection-With-YoloV3/" /> <meta name="description" content="Learn how to train a custom object detection with yolov3" /> <meta name="referrer" content="no-referrer-when-downgrade" /> <meta property="fb:app_id" content="" /> <meta property="og:site_name" content="Custom Object Detection With YoloV3 | Prabhat" /> <meta property="og:title" content="Custom Object Detection With YoloV3 | Prabhat" /> <meta property="og:type" content="website" /> <meta property="og:url" content="http://localhost:4000/Custom-Object-Detection-With-YoloV3/" /> <meta property="og:description" content="Learn how to train a custom object detection with yolov3" /> <meta property="og:image" content="http://localhost:4000/assets/favicons.svg" /> <meta property="og:image:width" content="640" /> <meta property="og:image:height" content="640" /> <meta name="twitter:card" content="summary" /> <meta name="twitter:title" content="Custom Object Detection With YoloV3 | caffeinedev" /> <meta name="twitter:url" content="http://localhost:4000/Custom-Object-Detection-With-YoloV3/" /> <meta name="twitter:site" content="@caffeinedev" /> <meta name="twitter:creator" content="@caffeinedev" /> <meta name="twitter:description" content="Learn how to train a custom object detection with yolov3" /> <meta name="twitter:image" content="http://localhost:4000/assets/favicons.svg" /> <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Prabhat" /> <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicons/coding-128" /> <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/coding-32.png" /> <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/coding-16.png" /> <!-- <link rel="manifest" href="/assets/favicons/site.webmanifest" /> <link rel="mask-icon" href="/assets/favicons/safari-pinned-tab.svg" color="#5bbad5" /> --> <meta name="apple-mobile-web-app-title" content="Prabhat" /> <meta name="application-name" content="Prabhat" /> <meta name="msapplication-TileColor" content="#da532c" /> <meta name="theme-color" content="#2c2c2c" /> <link rel="stylesheet" href="/assets/css/style.css" /> </head> <body data-theme="dark" class="notransition"> <script> const body = document.body; const data = body.getAttribute("data-theme"); const initTheme = (state) => { if (state === "dark") { body.setAttribute("data-theme", "dark"); } else if (state === "light") { body.removeAttribute("data-theme"); } else { localStorage.setItem("theme", data); } }; initTheme(localStorage.getItem("theme")); setTimeout(() => body.classList.remove("notransition"), 75); </script> <div class="navbar" role="navigation"> <nav class="menu"> <input type="checkbox" id="menu-trigger" class="menu-trigger" /> <label for="menu-trigger"> <span class="menu-icon"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <path d="M64,384H448V341.33H64Zm0-106.67H448V234.67H64ZM64,128v42.67H448V128Z" /> </svg> </span> </label> <a id="mode"> <svg class="mode-sunny" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <title>LIGHT</title> <line x1="256" y1="48" x2="256" y2="96" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="256" y1="416" x2="256" y2="464" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="108.92" x2="369.14" y2="142.86" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="369.14" x2="108.92" y2="403.08" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="464" y1="256" x2="416" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="96" y1="256" x2="48" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="403.08" x2="369.14" y2="369.14" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="142.86" x2="108.92" y2="108.92" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <circle cx="256" cy="256" r="80" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> </svg> <svg class="mode-moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <title>DARK</title> <line x1="256" y1="48" x2="256" y2="96" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="256" y1="416" x2="256" y2="464" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="108.92" x2="369.14" y2="142.86" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="369.14" x2="108.92" y2="403.08" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="464" y1="256" x2="416" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="96" y1="256" x2="48" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="403.08" x2="369.14" y2="369.14" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="142.86" x2="108.92" y2="108.92" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <circle cx="256" cy="256" r="80" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> </svg> </a> <div class="trigger"> <div class="trigger-container"><a class="menu-link" href="/">home</a><a class="menu-link" href="/blog/">blog</a><a class="menu-link" href="/uses/">uses</a><a class="menu-link" href="/bookshelf/">bookshelf</a><a class="menu-link" href="/about/">about</a><a class="menu-link rss" href="/feed.xml"> <svg xmlns="http://www.w3.org/2000/svg" width="17" height="17" viewBox="0 0 512 512" fill="#ED812E" > <title>RSS</title> <path d="M108.56,342.78a60.34,60.34,0,1,0,60.56,60.44A60.63,60.63,0,0,0,108.56,342.78Z" /> <path d="M48,186.67v86.55c52,0,101.94,15.39,138.67,52.11s52,86.56,52,138.67h86.66C325.33,312.44,199.67,186.67,48,186.67Z" /> <path d="M48,48v86.56c185.25,0,329.22,144.08,329.22,329.44H464C464,234.66,277.67,48,48,48Z" /> </svg> </a> </div> </div> </nav> </div> <div class="wrapper post"> <main class="page-content" aria-label="Content"> <article itemscope itemtype="https://schema.org/BlogPosting"> <header class="header"> <div class="tags"> <span itemprop="keywords"> <a class="tag" href="/tags/#yolov3">YOLOV3</a>, <a class="tag" href="/tags/#object-detection">OBJECT-DETECTION</a> </span> </div> <h1 class="header-title" itemprop="headline">Custom Object Detection With YoloV3</h1> <div class="post-meta"> <time datetime="2019-12-22T00:00:00+00:00" itemprop="datePublished"> Dec 22, 2019 </time> <span itemprop="author" itemscope itemtype="https://schema.org/Person"> <span itemprop="name">Prabhat</span> </span> <time hidden datetime="" itemprop="dateModified"> Dec 22, 2019 </time> <span hidden itemprop="publisher" itemtype="Person">Prabhat</span> <span hidden itemprop="image"></span> <span hidden itemprop="mainEntityOfPage"><p><img src="./bagpack.jpeg" alt="Cover image" /></p> </span> </div> </header> <div class="page-content" itemprop="articleBody"> <p><img src="./bagpack.jpeg" alt="Cover image" /></p> <p>Object Detection is a task in computer vision that focuses on detecting objects in images/videos.</p> <p>There are various object detection algorithms out there like YOLO (You Only Look Once,) Single Shot Detector (SSD), Faster R-CNN, Histogram of Oriented Gradients (HOG), etc.</p> <h3 id="prerequisites"> <a href="#prerequisites" class="anchor-head"></a> Prerequisites </h3> <p>It’s good to have a basic knowledge of deep learning computer vision. And basics of programming.</p> <p>In this article, we are going to train our own custom yolov3 model for object detection.</p> <p><strong>The steps needed are:</strong></p> <ol> <li>Gathering data</li> <li>Converting to YoloV3 format</li> <li>Setting Training Pipeline</li> <li>Training model</li> <li>Exporting weights file.</li> <li>Testing object detector</li> </ol> <h2 id="gathering-data"> <a href="#gathering-data" class="anchor-head"></a> Gathering data </h2> <p>We are going to get our data from <a href="https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&amp;type=detection&amp;c=%2Fm%2F0k5j">OpenImagesV5</a>. It was first introduced in 2016, It’s is a collaborative release comprising about nine million images annotated with labels covering thousands of object categories. The new version is an update on 2018’s <a href="https://storage.googleapis.com/openimages/web/factsfigures_v4.html">Open Images V4</a>.</p> <p>Open Image V5 features newly added annotations on image segmentation masks for 2.8 million objects in 350 categories. Unlike bounding-boxes that only identify the general area in which an object is located, these image segmentation masks trace the outline of the target object, characterizing it’s spatial extent with a higher level of detail.</p> <p>I am going to choose 4 classes of my choice from OpenImagesV5 that is <em>Backpack, Watch, Book, and Headphones</em>. You can choose as many as you want.</p> <p>We will have to download those images first. There’s an amazing <a href="https://github.com/pythonlessons/OIDv4_ToolKit">toolkit from GitHub</a> with a full explanation of how to use it. This toolkit will help us to download the images. The toolkit installation is easy. <a href="https://pylessons.com/YOLOv3-introduction/">This guy</a> has written a tutorial to detect custom objects using yolov3 using Keras. You can check it out, he has explained all the steps. In my post, I am going to use PyTorch and will try to simplify things as much as possible.</p> <ol> <li><strong>Downloading the toolkit</strong></li> </ol> <p>First, open up your terminal and clone the toolkit repository</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/pythonlessons/OIDv4_ToolKit.git
</code></pre></div></div> <p>Then go the folder by</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd OIDv4_ToolKit
</code></pre></div></div> <p>Install all the dependencies by</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div> <ol> <li><strong>Using toolkit</strong> <strong>&amp; Downloading the data</strong></li> </ol> <p>First of all how to check if we can download the appropriate image class we need? You need to go to the OIDv5 homepage, click on explore and in a search tab try to find your desired class. In my example, I will search for “Watch”, “Backpack”, “Book”, and “Headphones”. To download all of them you can simply run</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 main.py downloader <span class="nt">--classes</span> Watch Backpack Book Headphones <span class="nt">--type_csv</span> train <span class="nt">--multiclasses</span> 1 <span class="nt">--limit</span> 600
</code></pre></div></div> <p>With this command, 600 training images for each class will get downloaded. If you are using for the first time it will ask you to download (train-annotations-bbox or test-annotations-bbox) CSV file. Type Y, while running the command so that it would get downloaded.</p> <p><img src="./yolov3-1.png" alt="" /></p> <p>You need to wait for some time to finish the download.</p> <p><img src="./yolov3-2.png" alt="" /></p> <p>This is the folder structure :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>main_folder
│   - main.py
│   - oid_to_pascal_voc_xml.py
│   ..
└─── OID
    │
    └─── csv_folder
    │   │
    │   └─── class-descriptions-boxable.csv
    │   │
    │   └─── test-annotations-bbox.csv
    │   │
    │   └─── train-annotations-bbox.csv
    └─── OID
        │
        └─── Dataset
            │
            └─── train
                │
                └─── Watch_Backpack_Book_Headphones
</code></pre></div></div> <h2 id="converting-to-yolov3-format"> <a href="#converting-to-yolov3-format" class="anchor-head"></a> Converting to YoloV3 format </h2> <p>Open <code class="language-plaintext highlighter-rouge">oid_to_pascal_voc_xml.py</code> and edit line no. 9 and put</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>XML_DIR = 'TO_PASCAL_XML'
</code></pre></div></div> <p>Now you need to run</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 oid_to_pascal_voc_xml.py
</code></pre></div></div> <p>to generate the object co-ordinates in XML formats. After running this above file, you will get object label files in an XML format in the TO_PASCAL_XML folder.</p> <p>Again we need to convert it into YoloV3 format.</p> <p>To train a Yolo model there are requirements how an annotation file should be made:</p> <ul> <li>One row for one image;</li> <li>Row format: image_file_path box1 box2 … boxN;</li> <li>Box format: x_min,y_min,x_max,y_max,class_id (no space).</li> <li>Here is an example: <ul> <li>path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3</li> <li>path/to/img2.jpg 120,300,250,600,4 …</li> </ul> </li> </ul> <p>Now you will have to do some little bit of work with these. Have patience, it’s going to take some manual work now.</p> <p>I will use the code of <a href="https://github.com/ultralytics/yolov3">Ultralytics</a> to train our YoloV3 model. Make sure to check their repository, it’s great. I have made some changes in the folder structure and all for our own model. Now clone my repository for the next steps.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/TheCaffeineDev/YoloV3-Custom-Object-Detection.git
</code></pre></div></div> <p>This is the folder structure</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>main_folder
│   - detect.py
│   - models.py
│   - train.py
│ 	- test.py
│ 	- requirements.txt
│ 	...
└─── cfg
└─── conversion
    │
    └─── output
    └─── xmls
    │  - classes.txt
    │  - xmltotxt.py
    │  ...
└─── data
└─── training
│	│
│	└─── images
│	└─── labels
│	- object.names
│	- trainer.data
│	- yolov3.cfg
│	- train_test.py
│
└─── utils
└─── weights
</code></pre></div></div> <p>Now inside the main folder and run this in your terminal to install all the required dependencies.</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div> <p>Then you have to go back to that OIDV4_Toolkit folder <code class="language-plaintext highlighter-rouge">OIDv4_ToolKit/OID/Dataset/train/Watch_Backpack_Book_Headphones/</code> . Here you need to copy all the images into the following folder in our cloned repository <code class="language-plaintext highlighter-rouge">/YoloV3-Custom-Object-Detection/training/images</code> And</p> <p>you need to copy all the XML files which are in the folder to <code class="language-plaintext highlighter-rouge">/YoloV3-Custom-Object-Detection/conversion/xmls</code> folder. Don’t get confused about these above steps.</p> <p>After copying go to the <code class="language-plaintext highlighter-rouge">conversion</code> folder. You need to edit the <code class="language-plaintext highlighter-rouge">classes.txt </code> file and put your own classes there.</p> <p>It looks like this:</p> <p><img src="./yolov3-3.png" alt="" /></p> <p>Now in conversion folder run</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># In YoloV3-Custom-Object-Detection/conversion folder</span>
python3 xmltotxt.py <span class="nt">-xml</span> xmls <span class="nt">-out</span> output 
</code></pre></div></div> <p>This will generate all the label files in the yolov3 format inside the output folder.</p> <h2 id="setting-training-pipeline"> <a href="#setting-training-pipeline" class="anchor-head"></a> Setting Training Pipeline </h2> <p>Now you have to copy all the <code class="language-plaintext highlighter-rouge">.txt</code> files which are in <code class="language-plaintext highlighter-rouge">YoloV3-Custom-Object-Detection/conversion/output</code> folder to <code class="language-plaintext highlighter-rouge">YoloV3-Custom-Object-Detection/training/labels</code> folder.</p> <p>Now go to the <code class="language-plaintext highlighter-rouge">YoloV3-Custom-Object-Detection/training</code> folder. Now we will edit some files here. Now open that <code class="language-plaintext highlighter-rouge">object.names</code> file and edit it out with your own classes. Mine looks like this</p> <p>Now in</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># In YoloV3-Custom-Object-Detection/training folder</span>
python3 train_test.py
</code></pre></div></div> <p>This above file will generate <code class="language-plaintext highlighter-rouge">train.txt</code> and <code class="language-plaintext highlighter-rouge">test.txt</code>. You can open and check the file for more details. Basically we are splitting 70% &amp; 30% for training and testing respectively.</p> <p>Then open <code class="language-plaintext highlighter-rouge">trainer.data</code> . It looks like this. You need to put your no. of classes in the first line, <code class="language-plaintext highlighter-rouge">train.txt</code> and <code class="language-plaintext highlighter-rouge">test.txt</code> path in 2nd and 3rd line, <code class="language-plaintext highlighter-rouge">object.names</code> path in the 4th line.</p> <p>Mine looks like this</p> <p><img src="./yolov3-4.png" alt="" /></p> <p>Now you need to edit the <code class="language-plaintext highlighter-rouge">.cfg</code> file. By default each YOLO layer has 255 outputs: 85 outputs per anchor [4 box coordinates + 1 object confidence + 80 class confidences], times 3 anchors.</p> <p>In our case we are using only four classes, then we need to edit the filter. You can reduce filters to <code class="language-plaintext highlighter-rouge">filters=[4 + 1 + n] * 3</code>, where <code class="language-plaintext highlighter-rouge">n</code> is your class count. This modification should be made to the layer preceding each of the 3 YOLO layers. Also, modify <code class="language-plaintext highlighter-rouge">classes=80 </code> to <code class="language-plaintext highlighter-rouge">classes=n</code> in each YOLO layer, where <code class="language-plaintext highlighter-rouge">n</code> is your class count.</p> <p>In our case in <code class="language-plaintext highlighter-rouge">yolov3.cfg</code> file I changed the batch size &amp; subdivisions which is in line no. <strong>6</strong> and <strong>7</strong>. Then line no <strong>610</strong> (classes=4) and <strong>603</strong> (filters=27), then line no. <strong>689</strong> &amp; <strong>696</strong>, lastly line no. <strong>776</strong> &amp; <strong>783</strong>. If you are using, line no. will be different.</p> <p><img src="./yolov3-5.png" alt="" /></p> <p>Now we are ready to train our yolov3 model.</p> <h2 id="training-model"> <a href="#training-model" class="anchor-head"></a> Training Model </h2> <p>Below is my desktop specification in which I am going to train my model.</p> <ul> <li><strong>GPU:</strong> NVIDIA GeForce RTX <em>2080 SUPER</em> (8GB)</li> <li><strong>RAM:</strong> 16GB DDR4</li> <li>OS: Ubuntu 18.04</li> </ul> <p>Now in <code class="language-plaintext highlighter-rouge">YoloV3-Custom-Object-Detection</code> folder open terminal and run</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># In YoloV3-Custom-Object-Detection do</span>
python3 train.py <span class="nt">--epochs</span> 110 <span class="nt">--data</span> training/trainer.data <span class="nt">--cfg</span> training/yolov3.cfg <span class="nt">--batch</span> 16 <span class="nt">--accum</span> 1 
</code></pre></div></div> <p>There are optional arguments are there, you can check-in <code class="language-plaintext highlighter-rouge">train.py</code> file.</p> <p>Also, You can <strong>Update hyperparameters</strong> also such as LR, LR scheduler, optimizer, augmentation settings, multi_scale settings, etc in <code class="language-plaintext highlighter-rouge">train.py</code> for your particular task. I would recommend you start with all-default settings first updating anything.</p> <p><img src="./yolov3-6.png" alt="" /></p> <p>To run tensorboard while training, open another terminal and run</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># In YoloV3-Custom-Object-Detection do</span>
tensorboard <span class="nt">--logdir</span><span class="o">=</span>runs
</code></pre></div></div> <p><img src="./yolov3-7.png" alt="" /></p> <p>It took me 2 hours to train four objects with 110 epochs.</p> <p><img src=".//yolov3-8.png" alt="" /></p> <p>If you follow the above steps, you will be able to train your own model properly.</p> <h2 id="exporting-weights-file"> <a href="#exporting-weights-file" class="anchor-head"></a> Exporting weights file </h2> <p>After training the model, we can get the weights file in the weights folder. It’s in <code class="language-plaintext highlighter-rouge">PyTorch</code> model format. We need to convert it to <code class="language-plaintext highlighter-rouge">darknet</code> format to test it out. Let’s convert it first. Open up the terminal and run</p> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># convert cfg/pytorch model to darknet weights</span>
<span class="c"># In YoloV3-Custom-Object-Detection</span>
python3  <span class="nt">-c</span> <span class="s2">"from models import *; convert('training/yolov3.cfg', 'weights/best.pt')"</span>

</code></pre></div></div> <p>You will get this message after conversion.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Success: converted 'weights/yolov3-spp.pt' to 'converted.weights'
</code></pre></div></div> <p>Now we will test our object detector. You will have the <code class="language-plaintext highlighter-rouge">converted.weights </code> in <code class="language-plaintext highlighter-rouge">YoloV3-Custom-Object-Detection</code> folder.</p> <h2 id="testing-object-detector"> <a href="#testing-object-detector" class="anchor-head"></a> Testing Object Detector </h2> <p>Now to run inference, open up your terminal and run</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># In YoloV3-Custom-Object-Detection</span>
python3 detect.py <span class="nt">--source</span> 0 <span class="nt">--weights</span> converted.weights <span class="nt">--cfg</span> training/yolov3.cfg <span class="nt">--names</span> training/object.names <span class="nt">--img-size</span> 416
</code></pre></div></div> <p>Check <code class="language-plaintext highlighter-rouge">detect.py</code> for details.</p> <ul> <li>Image: ` –source file.jpg `</li> <li>Video: ` –source file.mp4 `</li> <li>Directory: ` –source dir/ `</li> <li>Webcam: <code class="language-plaintext highlighter-rouge">--source 0 </code></li> <li>RTSP stream: <code class="language-plaintext highlighter-rouge">--source rtsp://192.168.0.1/rtp/470011235daa </code></li> </ul> <p>Some of the image inferences below you can see:</p> <p><img src="./yolov3-9.jpeg" alt="" /></p> <p><img src="./yolov3-10.jpeg" alt="" /></p> <p>Due to the GitHub file size limit, I couldn’t upload it to GitHub. If you want to test my weight file, you can get it by</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://www.dropbox.com/s/tou8f0xvs8wdt8a/converted.weights
</code></pre></div></div> <p>Link to the repository: https://github.com/TheCaffeineDev/YoloV3-Custom-Object-Detection</p> <p>If you want to label your own custom images you can use <a href="https://github.com/tzutalin/labelImg">LabelImg</a>.The process of training and all is almost the same.</p> <p>If you have any questions, recommendations or critiques, I can be reached via <a href="https://twitter.com/thecaffeinedev">Twitter</a> or via my <a href="mailto:iprabhatdev@gmail.com">mail</a>. Feel free to reach out to me.</p> <p><strong><em>Thank You</em></strong></p> <h3 id="references-and-credit"> <a href="#references-and-credit" class="anchor-head"></a> References and Credit </h3> <ol> <li><a href="https://medium.com/syncedreview/google-releases-open-image-v5-launches-open-image-challenge-91fa802f0edf">Medium blog</a></li> <li><a href="https://github.com/ultralytics/yolov3">Ultralytics repo</a></li> <li><a href="https://pylessons.com/">Pylessons</a></li> </ol> </div> </article> </main> <nav class="post-nav"> <a class="post-nav-item post-nav-prev" href="/Custom-Object-Detection-With-Tensorflow-Object-Detection-API/" > <div class="nav-arrow">Previous</div> <span class="post-title">Custom Object Detection With Tensorflow Object Detection API</span> </a> <a class="post-nav-item post-nav-next" href="/customize-your-terminal-on-ubuntu/"> <div class="nav-arrow">Next</div> <span class="post-title">Customize Your Terminal On Ubuntu</span> </a> </nav> <footer class="footer"> <a class="footer_item" href="/thanks">ack.</a> <a class="footer_item" href="/feed.xml">rss</a> <span class="footer_item">&copy; 2022</span> <small class="footer_copyright"> <a href="https://github.com/TheCaffeineDev" target="_blank" rel="noreferrer noopener" >@Caffeinedev</a > </small> </footer> <script src="/assets/js/main.js" defer="defer"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-150491781-1"></script> <script> window['ga-disable-UA-150491781-1'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1"; window.dataLayer = window.dataLayer || []; function gtag(){window.dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-150491781-1'); </script><!-- <script src="/assets/js/galite.js"></script> <script> var galite = galite || {}; galite.UA = "UA-150491781-1"; </script> --> </div> </body> </html>
